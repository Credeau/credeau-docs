{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Credeau Product Documentation","text":"<p>Get started with Credeau\u2019s products using our comprehensive guides and technical documentation. Whether you're integrating, deploying, or exploring features \u2014 everything you need is right here.</p>"},{"location":"#available-products","title":"Available Products","text":"<ul> <li>MobileForge</li> <li>MobileGator</li> <li>CredForge</li> </ul>"},{"location":"products/credforge/","title":"CredForge","text":"<p>CredForge is a powerful decision-making platform that streamlines and accelerates the credit assessment process by eliminating technology barriers and enabling real-time decisioning capabilities.</p> <p>CredForge aims to take the technology lag out of decision making process by providing:</p> <ul> <li>Configurable Integrations to external APIs for seamless data flow</li> <li>Python script upload capabilities for custom feature and score creation</li> <li>On-the-fly rule modification for dynamic decisioning</li> <li>End-to-end decisioning workflow testing and validation</li> </ul>"},{"location":"products/credforge/#key-features","title":"Key Features","text":"<ul> <li>Flexible integration framework for external systems</li> <li>Customizable scoring and feature engineering</li> <li>Real-time rule engine for dynamic decisioning</li> <li>Comprehensive testing and validation tools</li> <li>Secure and compliant data handling</li> <li>Scalable architecture for high-volume processing</li> </ul> <p>Explore the sections below to learn more about CredForge's components, integration capabilities, rule engine, and deployment strategies.</p>"},{"location":"products/credforge/#architecture-overview","title":"Architecture Overview","text":"<p>The diagram below illustrates a typical deployment of the CredForge platform. It is intended as a reference architecture showcasing how various components work together in a production-grade setup.</p> <p></p> <p>This architecture highlights key components such as external API integrations, rule engine processing, feature computation, and the decisioning workflow that enables real-time credit assessment and decisioning.</p>"},{"location":"products/credforge/components/","title":"CredForge Components","text":"<p>CredForge is built on two primary components - the Config Manager and the Core APIs, each composed of modular services that can be independently scaled and deployed.</p>"},{"location":"products/credforge/components/#core-apis","title":"Core APIs","text":"<p>Exposes core APIs that orchestrate the decisioning workflow by integrating workflow management, rules processing, feature computation, and external system connectivity.</p> <p></p> <p>Components:</p> <ul> <li> <p>Workflow Manager: Orchestrates the decisioning process by executing predefined workflows against user input variables, ensuring proper sequencing and state management.</p> </li> <li> <p>Rule Engine: Evaluates business rules and conditions through a configurable decision tree, enabling complex decision logic and policy enforcement.</p> </li> <li> <p>Feature Engine: Processes input variables through configurable feature computation pipelines, transforming raw data into meaningful decisioning features.</p> </li> <li> <p>External Connection Manager: Manages and orchestrates integrations with external services through configurable connection profiles and metadata-driven configurations.</p> </li> <li> <p>Databases:</p> <ul> <li>MongoDB: Document database for storing system configurations, workflow definitions, rule sets, and integration metadata across all core decisioning components.</li> <li>PostgreSQL: Relational database for managing transactional data, audit trails, and application logs with robust data integrity and query capabilities.</li> </ul> </li> </ul>"},{"location":"products/credforge/components/#config-manager","title":"Config Manager","text":"<p>\u2b50 <code>Releasing Soon, currently powered by Git. Follow below documentation for more.</code></p> <p>Responsible for orchestrating and managing the core decisioning components including workflow configuration, rules engine, feature computation engine, and external system integrations.</p> <p>Components:</p> <ul> <li> <p>Admin Portal UI: Provides an intuitive web interface for administrators to manage system configurations, monitor workflows, and control decisioning parameters.</p> </li> <li> <p>Admin Portal Backend: Exposes RESTful APIs to handle configuration management, export settings, and maintain system state across the decisioning platform.</p> </li> <li> <p>Databases:</p> <ul> <li>MongoDB: Document database for storing versioned configurations, system metadata, and portal settings with support for configuration history and rollback capabilities.</li> </ul> </li> </ul> <p>Components (Legacy deployments):</p> <p>Client-managed Git repositories that store versioned configuration files for each core API component, enabling version control and deployment management.</p> <ul> <li>Git Repositories:<ul> <li><code>cred-forge-wm-configs</code></li> <li><code>cred-forge-re-configs</code></li> <li><code>cred-forge-fe-configs</code></li> <li><code>cred-forge-ecm-configs</code></li> </ul> </li> </ul> <p>Each component is designed to be modular, allowing independent scaling, deployment, and monitoring depending on workload or use-case.</p>"},{"location":"products/credforge/deployment/coreapis/","title":"Deployment: CredForge Core APIs","text":""},{"location":"products/credforge/deployment/database/","title":"Deployment: CredForge Databases","text":"<p>This document outlines the deployment steps for the required databases: MongoDB 7.0 and PostgreSQL 16.</p>"},{"location":"products/credforge/deployment/database/#database-requirements","title":"Database Requirements","text":"<ul> <li>MongoDB 7.0</li> <li>PostgreSQL 16</li> </ul>"},{"location":"products/credforge/deployment/database/#deployment-methods","title":"Deployment Methods","text":""},{"location":"products/credforge/deployment/database/#1-docker-compose-recommended-for-dev-testing-environments","title":"1. Docker Compose (Recommended for Dev Testing Environments)","text":""},{"location":"products/credforge/deployment/database/#mongodb-70","title":"MongoDB 7.0","text":"<pre><code>version: '3.8'\nservices:\n  mongodb:\n    image: mongo:7.0\n    container_name: mongodb\n    ports:\n      - \"27017:27017\"\n    environment:\n      - MONGO_INITDB_ROOT_USERNAME=admin\n      - MONGO_INITDB_ROOT_PASSWORD=your_secure_password\n    volumes:\n      - mongodb_data:/data/db\n\nvolumes:\n  mongodb_data:\n</code></pre>"},{"location":"products/credforge/deployment/database/#postgresql-16","title":"PostgreSQL 16","text":"<pre><code>version: '3.8'\nservices:\n  postgres:\n    image: postgres:16\n    container_name: postgres\n    ports:\n      - \"5432:5432\"\n    environment:\n      - POSTGRES_USER=admin\n      - POSTGRES_PASSWORD=your_secure_password\n      - POSTGRES_DB=postgres\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\nvolumes:\n  postgres_data:\n</code></pre> <p>To deploy using Docker Compose:</p> <ol> <li>Create a <code>docker-compose.yml</code> file with both services</li> <li>Run <code>docker-compose up -d</code></li> <li>Verify services are running with <code>docker-compose ps</code></li> </ol>"},{"location":"products/credforge/deployment/database/#2-custom-installation-on-ubuntu-2204-recommended-for-uat-environments","title":"2. Custom Installation on Ubuntu 22.04 (Recommended for UAT Environments)","text":""},{"location":"products/credforge/deployment/database/#mongodb-70-installation","title":"MongoDB 7.0 Installation","text":"<p>Import MongoDB public GPG key: <pre><code>curl -fsSL https://pgp.mongodb.com/server-7.0.asc | \\\n   sudo gpg -o /usr/share/keyrings/mongodb-server-7.0.gpg \\\n   --dearmor\n</code></pre></p> <p>Create list file for MongoDB: <pre><code>echo \"deb [ arch=amd64,arm64 signed-by=/usr/share/keyrings/mongodb-server-7.0.gpg ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/7.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-7.0.list\n</code></pre></p> <p>Update package database and install MongoDB: <pre><code>sudo apt-get update\nsudo apt-get install -y mongodb-org\n</code></pre></p> <p>Start and enable MongoDB: <pre><code>sudo systemctl start mongod\nsudo systemctl enable mongod\n</code></pre></p>"},{"location":"products/credforge/deployment/database/#postgresql-16-installation","title":"PostgreSQL 16 Installation","text":"<p>Add PostgreSQL repository: <pre><code>sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" &gt; /etc/apt/sources.list.d/pgdg.list'\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\n</code></pre></p> <p>Install PostgreSQL: <pre><code>sudo apt-get update\nsudo apt-get install -y postgresql-16\n</code></pre></p> <p>Start and enable PostgreSQL: <pre><code>sudo systemctl start postgresql\nsudo systemctl enable postgresql\n</code></pre></p>"},{"location":"products/credforge/deployment/database/#3-aws-managed-databases-recommended-for-production-environments","title":"3. AWS Managed Databases (Recommended for Production Environments)","text":""},{"location":"products/credforge/deployment/database/#amazon-documentdb-mongodb-compatible","title":"Amazon DocumentDB (MongoDB Compatible)","text":"<ol> <li>Create a DocumentDB cluster:</li> <li>Go to AWS Console \u2192 DocumentDB</li> <li>Click \"Create cluster\"</li> <li>Select engine version compatible with MongoDB 7.0</li> <li>Configure instance class (recommended: db.r6g.large or higher)</li> <li>Set up VPC and security groups</li> <li>Configure backup retention period</li> <li> <p>Set up encryption at rest</p> </li> <li> <p>Configure security:</p> </li> <li>Configure security groups to allow access from application servers</li> </ol>"},{"location":"products/credforge/deployment/database/#amazon-rds-for-postgresql-16","title":"Amazon RDS for PostgreSQL 16","text":"<ol> <li>Create an RDS instance:</li> <li>Go to AWS Console \u2192 RDS</li> <li>Click \"Create database\"</li> <li>Choose PostgreSQL 16</li> <li>Select instance class (recommended: db.r6g.large or higher)</li> <li>Configure storage (recommended: 100GB+ with autoscaling)</li> <li>Set up VPC and security groups</li> <li>Enable Multi-AZ deployment for high availability</li> <li>Configure backup retention period</li> <li> <p>Enable encryption at rest</p> </li> <li> <p>Configure security:</p> </li> <li>Configure security groups to allow access from application servers</li> </ol>"},{"location":"products/credforge/deployment/database/#database-schema-setup","title":"Database Schema Setup","text":""},{"location":"products/credforge/deployment/database/#postgresql-database-setup","title":"PostgreSQL Database Setup","text":""},{"location":"products/credforge/deployment/database/#required-postgresql-databases","title":"Required PostgreSQL Databases","text":"<ul> <li><code>cred_forge_db</code></li> </ul>"},{"location":"products/credforge/deployment/database/#create-database-and-user","title":"Create database and user:","text":"<pre><code>-- Create databases\nCREATE DATABASE cred_forge_db;\n\n-- Create user with password\nCREATE USER credforge_user WITH PASSWORD 'your_secure_password';\n\n-- Grant privileges for api_insights_db\nGRANT CONNECT ON DATABASE cred_forge_db TO credforge_user;\nGRANT USAGE ON SCHEMA public TO credforge_user;\nGRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO credforge_user;\nGRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA public TO credforge_user;\n\n-- Set default privileges for future tables in both databases\nALTER DEFAULT PRIVILEGES IN SCHEMA public \nGRANT SELECT, INSERT, UPDATE, DELETE ON TABLES TO credforge_user;\n\nALTER DEFAULT PRIVILEGES IN SCHEMA public \nGRANT USAGE, SELECT ON SEQUENCES TO credforge_user;\n</code></pre>"},{"location":"products/credforge/deployment/database/#create-tables","title":"Create Tables","text":"<p>Database to use - <code>cred_forge_db</code></p> <pre><code>CREATE TABLE forge_request_workflow_state (\n    request_id VARCHAR(255) PRIMARY KEY,\n    user_id VARCHAR(255),\n    reference_id VARCHAR(255),\n    client_id VARCHAR(255),\n    workflow_endpoint VARCHAR(255),\n    workflow_strategy VARCHAR(255),\n    workflow_config JSONB,\n    input_json JSONB DEFAULT '{}'::jsonb,\n    ecm JSONB,\n    features JSONB,\n    rules_output JSONB,\n    engine_history JSONB,\n    created_at TIMESTAMP NOT NULL,\n    created_date TIMESTAMP NOT NULL,\n    updated_at TIMESTAMP NOT NULL\n);\n\n-- Optional indices for better query performance\nCREATE INDEX idx_request_workflow_state_reference_id ON forge_request_workflow_state(reference_id);\nCREATE INDEX idx_request_workflow_state_user_id ON forge_request_workflow_state(user_id);\nCREATE INDEX idx_request_workflow_state_request_id ON forge_request_workflow_state(request_id);\nCREATE INDEX idx_request_workflow_state_client_id ON forge_request_workflow_state(client_id);\nCREATE INDEX idx_request_workflow_state_workflow_endpoint ON forge_request_workflow_state(workflow_endpoint);\nCREATE INDEX idx_request_workflow_state_workflow_strategy ON forge_request_workflow_state(workflow_strategy);\nCREATE INDEX idx_request_workflow_state_created_date ON forge_request_workflow_state(created_date);\n\nCREATE TABLE forge_ecm_response_log (\n    id SERIAL PRIMARY KEY,\n    request_id VARCHAR(255) NOT NULL,\n    user_id VARCHAR(255),\n    reference_id VARCHAR(255),\n    engine VARCHAR(255),\n    client_id VARCHAR(255),\n    external_status VARCHAR(255),\n    external_response_code INTEGER,\n    external_retries INTEGER,\n    external_latency FLOAT,\n    ecm_data_source VARCHAR(255),\n    ecm_message VARCHAR(255),\n    metadata_json JSONB,\n    ecm_status_code INTEGER,\n    created_at TIMESTAMP NOT NULL,\n    created_date TIMESTAMP NOT NULL\n);\n\n-- Indices for better query performance\nCREATE INDEX idx_ecm_response_log_request_id ON forge_ecm_response_log(request_id);\nCREATE INDEX idx_ecm_response_log_reference_id ON forge_ecm_response_log(reference_id);\nCREATE INDEX idx_ecm_response_log_user_id ON forge_ecm_response_log(user_id);\nCREATE INDEX idx_ecm_response_log_engine ON forge_ecm_response_log(engine);\nCREATE INDEX idx_ecm_response_log_created_date ON forge_ecm_response_log(created_date);\n\nCREATE TABLE forge_application_logs (\n    id CHAR(36),\n    request_id VARCHAR(255),\n    client_id VARCHAR(255),\n    reference_id VARCHAR(255),\n    user_id VARCHAR(255),\n    forge_component VARCHAR(255) NOT NULL,\n    api_endpoint VARCHAR(255),\n    engine_name VARCHAR(255),\n    workflow_name VARCHAR(255),\n    level VARCHAR(50) NOT NULL,\n    timestamp TIMESTAMP NOT NULL,\n    message TEXT NOT NULL,\n    extra_data JSON,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    PRIMARY KEY (timestamp, id)\n) ENGINE=InnoDB\nPARTITION BY RANGE (UNIX_TIMESTAMP(timestamp)) (\n    PARTITION p_first VALUES LESS THAN (UNIX_TIMESTAMP('2024-01-01 00:00:00'))\n);\n\n-- Create indexes\nCREATE INDEX idx_application_logs_request_id ON forge_application_logs(request_id);\nCREATE INDEX idx_application_logs_client_id ON forge_application_logs(client_id);\nCREATE INDEX idx_application_logs_reference_id ON forge_application_logs(reference_id);\nCREATE INDEX idx_application_logs_user_id ON forge_application_logs(user_id);\nCREATE INDEX idx_application_logs_forge_component ON forge_application_logs(forge_component);\nCREATE INDEX idx_application_logs_api_endpoint ON forge_application_logs(api_endpoint);\nCREATE INDEX idx_application_logs_engine_name ON forge_application_logs(engine_name);\nCREATE INDEX idx_application_logs_workflow_name ON forge_application_logs(workflow_name);\nCREATE INDEX idx_application_logs_level ON forge_application_logs(level);\n</code></pre>"},{"location":"products/credforge/deployment/database/#mongodb-database-setup","title":"MongoDB Database Setup","text":""},{"location":"products/credforge/deployment/database/#create-databases-and-user","title":"Create databases and user","text":"<pre><code>// Connect to MongoDB\nuse admin\n\n// Create application user\ndb.createUser({\n  user: \"credforge_user\",\n  pwd: \"your_secure_password\",\n  roles: [\n    { role: \"readWrite\", db: \"cred_forge_db\" }\n  ]\n})\n</code></pre>"},{"location":"products/credforge/deployment/database/#create-collections-in-mongodb","title":"Create collections in MongoDB","text":"<pre><code>// 1. Clients\ndb.createCollection(\"clients\");\ndb.clients.createIndex({ \"client_id\": 1 });\ndb.clients.createIndex({ \"auth_token\": 1 });\n\n// 2. Ecm Client Metadata\ndb.createCollection(\"ecm_client_metadata\");\ndb.ecm_client_metadata.createIndex({ \"client_id\": 1 });\ndb.ecm_client_metadata.createIndex({ \"engine\": 1 });\n\n// 3. Ecm Output Log\ndb.createCollection(\"ecm_output_log\");\ndb.ecm_output_log.createIndex({ \"reference_id\": 1 });\ndb.ecm_output_log.createIndex({ \"user_id\": 1 });\ndb.ecm_output_log.createIndex({ \"engine\": 1 });\ndb.ecm_output_log.createIndex({ \"request_id\": 1 });\ndb.ecm_output_log.createIndex({ \"client_id\": 1 });\n\n// 4. Ecm Request Log\ndb.createCollection(\"ecm_request_log\");\ndb.ecm_request_log.createIndex({ \"reference_id\": 1 });\ndb.ecm_request_log.createIndex({ \"user_id\": 1 });\ndb.ecm_request_log.createIndex({ \"request_id\": 1 });\ndb.ecm_request_log.createIndex({ \"engine\": 1 });\ndb.ecm_request_log.createIndex({ \"client_id\": 1 });\n\n// 5. Ecm Response Log\ndb.createCollection(\"ecm_response_log\");\ndb.ecm_response_log.createIndex({ \"reference_id\": 1 });\ndb.ecm_response_log.createIndex({ \"request_id\": 1 });\ndb.ecm_response_log.createIndex({ \"user_id\": 1 });\ndb.ecm_response_log.createIndex({ \"engine\": 1 });\ndb.ecm_response_log.createIndex({ \"client_id\": 1 });\n\n// 6. Request Workflow State\ndb.createCollection(\"request_workflow_state\");\ndb.request_workflow_state.createIndex({ \"client_id\": 1 });\ndb.request_workflow_state.createIndex({ \"created_date\": 1 });\ndb.request_workflow_state.createIndex({ \"user_id\": 1 });\ndb.request_workflow_state.createIndex({ \"request_id\": 1 });\ndb.request_workflow_state.createIndex({ \"reference_id\": 1 });\ndb.request_workflow_state.createIndex({ \"workflow_strategy\": 1 });\ndb.request_workflow_state.createIndex({ \"workflow_endpoint\": 1 });\n\n// 7. Unauthorized Requests\ndb.createCollection(\"unauthorized_requests\");\ndb.unauthorized_requests.createIndex({ \"client_id\": 1 });\ndb.unauthorized_requests.createIndex({ \"request_id\": 1 });\n\n// 8. Usage\ndb.createCollection(\"usage\");\ndb.usage.createIndex({ \"client_id\": 1 });\ndb.usage.createIndex({ \"request_id\": 1 });\ndb.usage.createIndex({ \"api_name\": 1 });\n\n// 9. Workflow Configs\ndb.createCollection(\"workflow_configs\");\ndb.workflow_configs.createIndex({ \"workflow_name\": 1 });\ndb.workflow_configs.createIndex({ \"client_id\": 1 });\n\n// 10. Workflow Endpoint Config\ndb.createCollection(\"workflow_endpoint_config\");\ndb.workflow_endpoint_config.createIndex({ \"workflow_endpoint\": 1 });\ndb.workflow_endpoint_config.createIndex({ \"client_id\": 1 });\n\n// 11. Workflow Event Log\ndb.createCollection(\"workflow_event_log\");\ndb.workflow_event_log.createIndex({ \"client_id\": 1 });\ndb.workflow_event_log.createIndex({ \"reference_id\": 1 });\ndb.workflow_event_log.createIndex({ \"request_id\": 1 });\ndb.workflow_event_log.createIndex({ \"user_id\": 1 });\ndb.workflow_event_log.createIndex({ \"event_type\": 1 });\n</code></pre>"},{"location":"products/credforge/deployment/database/#important-notes","title":"Important Notes","text":"<ol> <li>Replace <code>your_secure_password</code> with a strong, secure password in all scripts</li> <li>Ensure proper network security rules are in place before running these scripts</li> <li>For production environments, use AWS Secrets Manager or similar services to manage credentials</li> <li>Regularly rotate database credentials</li> <li>Monitor database access logs for any unauthorized access attempts</li> </ol>"},{"location":"products/credforge/deployment/database/#security-considerations","title":"Security Considerations","text":"<ol> <li>Always use strong passwords</li> <li>Enable encryption at rest</li> <li>Configure network security (firewalls, security groups)</li> <li>Regular backups</li> <li>Monitor database performance and logs</li> <li>Keep databases updated with security patches</li> </ol>"},{"location":"products/credforge/deployment/database/#backup-and-recovery","title":"Backup and Recovery","text":""},{"location":"products/credforge/deployment/database/#mongodb","title":"MongoDB","text":"<ul> <li>Regular automated backups</li> <li>Point-in-time recovery (for DocumentDB)</li> <li>Backup verification</li> <li>Recovery testing</li> </ul>"},{"location":"products/credforge/deployment/database/#postgresql","title":"PostgreSQL","text":"<ul> <li>Automated snapshots</li> <li>Point-in-time recovery</li> <li>WAL archiving</li> <li>Backup verification</li> <li>Recovery testing</li> </ul>"},{"location":"products/credforge/deployment/database/#monitoring","title":"Monitoring","text":"<ol> <li>Set up CloudWatch metrics (for AWS deployments)</li> <li>Configure database-specific monitoring tools</li> <li>Set up alerts for:</li> <li>High CPU usage</li> <li>Low disk space</li> <li>Connection count</li> <li>Error rates</li> <li>Replication lag</li> </ol>"},{"location":"products/credforge/deployment/database/#maintenance","title":"Maintenance","text":"<ol> <li>Regular security updates</li> <li>Performance optimization</li> <li>Index maintenance</li> <li>Vacuum operations (PostgreSQL)</li> <li>Database statistics updates</li> </ol>"},{"location":"products/credforge/deployment/database/#scaling-ladder","title":"Scaling Ladder","text":"<p>The following table provides recommended node specifications and counts for PostgreSQL and MongoDB deployments at various Daily Active User (DAU) scales.</p> <ul> <li>PostgreSQL: 1GB RAM, 2 vCPU, 25GB disk (3000 IOPS, 125 MBPS)</li> <li>MongoDB: 4GB RAM, 2 vCPU, 100GB disk (3000 IOPS, 125 MBPS)</li> </ul> Requests / Day PostgreSQL Nodes MongoDB Nodes 10K 1 1 25K 2 2 50K 3 3 75K 4 4 100K 5 5 <p>Notes:</p> <ul> <li>For high availability and failover, consider running at least 2 nodes (primary + standby/replica) at all times, even at lower DAU.</li> <li>Scale up node count and/or instance size if you observe CPU, RAM, or disk IOPS/throughput consistently above 50% utilization.</li> <li>Adjust disk size and IOPS as data volume grows.</li> <li>Monitor database metrics and tune accordingly.</li> </ul>"},{"location":"products/mobileforge/","title":"MobileForge","text":"<p>MobileForge ensures that all data collected from user devices is securely transmitted directly to the institution\u2019s infrastructure, where profiling and processing take place \u2014 maintaining full control and compliance within institutional premises.</p>"},{"location":"products/mobileforge/#key-features","title":"Key Features","text":"<ul> <li>Modular architecture</li> <li>Scalable deployment options</li> <li>Robust data syncing with minimal drops</li> <li>Powerful data features and insights for user profiling  </li> <li>Utilizes open-source databases (MongoDB and PostgreSQL) for efficient and reliable data storage</li> </ul> <p>Explore the sections below to learn more about MobileForge's components, deployment strategies, scaling guidelines, and data schemas.</p>"},{"location":"products/mobileforge/#architecture-overview","title":"Architecture Overview","text":"<p>The diagram below illustrates a typical deployment of the MobileForge platform on AWS. It is intended as a reference architecture showcasing how various components work together in a production-grade setup.</p> <p></p> <p>This architecture highlights key components such as data ingestion from mobile devices, secure transmission to institutional infrastructure, robust data syncing, and the combined use of MongoDB and PostgreSQL for optimized storage and profiling operations.</p>"},{"location":"products/mobileforge/components/","title":"MobileForge Components","text":"<p>MobileForge is built on two primary pipelines \u2014 the Sync Pipeline and the Insights Pipeline, each composed of modular services that can be independently scaled and deployed.</p>"},{"location":"products/mobileforge/components/#sync-pipeline","title":"Sync Pipeline","text":"<p>Responsible for securely ingesting data from mobile devices and storing it for downstream processing.</p> <p></p> <p>Components:</p> <ul> <li> <p>Producer API: Receives raw data from mobile clients.</p> </li> <li> <p>Kafka Stream: Serves as the backbone for streaming data reliably between services.</p> </li> <li> <p>Data Consumers: Process and store incoming data from Kafka.</p> <ul> <li>SMS Data Consumer</li> <li>Events Data Consumer</li> <li>Device/Apps Data Consumer</li> </ul> </li> <li> <p>Databases (Write Layer):</p> <ul> <li>MongoDB: Used for handling unstructured or semi-structured data.</li> <li>PostgreSQL: Stores structured and relational data with integrity constraints.</li> </ul> </li> </ul>"},{"location":"products/mobileforge/components/#insights-pipeline","title":"Insights Pipeline","text":"<p>Provides APIs for reading, profiling, and deriving insights from the synced data.</p> <p></p> <p>Components:</p> <ul> <li> <p>Insights API: Delivers processed insights to institutional systems.</p> </li> <li> <p>SMS Extraction API: Extracts relevant financial signals from SMS data.</p> </li> <li> <p>Databases (Read Layer):</p> <ul> <li>MongoDB: Separate database instance within the same cluster, optimized for read access.</li> <li>PostgreSQL: Read-optimized schema for efficient querying and reporting.</li> </ul> </li> </ul> <p>Each component is designed to be modular, allowing independent scaling, deployment, and monitoring depending on workload or use-case.</p>"},{"location":"products/mobileforge/deployment/database/","title":"Deployment: MobileForge Databases","text":"<p>This document outlines the deployment steps for the required databases: MongoDB 7.0 and PostgreSQL 16.</p>"},{"location":"products/mobileforge/deployment/database/#database-requirements","title":"Database Requirements","text":"<ul> <li>MongoDB 7.0</li> <li>PostgreSQL 16</li> </ul>"},{"location":"products/mobileforge/deployment/database/#deployment-methods","title":"Deployment Methods","text":""},{"location":"products/mobileforge/deployment/database/#1-docker-compose-recommended-for-dev-testing-environments","title":"1. Docker Compose (Recommended for Dev Testing Environments)","text":""},{"location":"products/mobileforge/deployment/database/#mongodb-70","title":"MongoDB 7.0","text":"<pre><code>version: '3.8'\nservices:\n  mongodb:\n    image: mongo:7.0\n    container_name: mongodb\n    ports:\n      - \"27017:27017\"\n    environment:\n      - MONGO_INITDB_ROOT_USERNAME=admin\n      - MONGO_INITDB_ROOT_PASSWORD=your_secure_password\n    volumes:\n      - mongodb_data:/data/db\n\nvolumes:\n  mongodb_data:\n</code></pre>"},{"location":"products/mobileforge/deployment/database/#postgresql-16","title":"PostgreSQL 16","text":"<pre><code>version: '3.8'\nservices:\n  postgres:\n    image: postgres:16\n    container_name: postgres\n    ports:\n      - \"5432:5432\"\n    environment:\n      - POSTGRES_USER=admin\n      - POSTGRES_PASSWORD=your_secure_password\n      - POSTGRES_DB=postgres\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\nvolumes:\n  postgres_data:\n</code></pre> <p>To deploy using Docker Compose:</p> <ol> <li>Create a <code>docker-compose.yml</code> file with both services</li> <li>Run <code>docker-compose up -d</code></li> <li>Verify services are running with <code>docker-compose ps</code></li> </ol>"},{"location":"products/mobileforge/deployment/database/#2-custom-installation-on-ubuntu-2204-recommended-for-uat-environments","title":"2. Custom Installation on Ubuntu 22.04 (Recommended for UAT Environments)","text":""},{"location":"products/mobileforge/deployment/database/#mongodb-70-installation","title":"MongoDB 7.0 Installation","text":"<p>Import MongoDB public GPG key: <pre><code>curl -fsSL https://pgp.mongodb.com/server-7.0.asc | \\\n   sudo gpg -o /usr/share/keyrings/mongodb-server-7.0.gpg \\\n   --dearmor\n</code></pre></p> <p>Create list file for MongoDB: <pre><code>echo \"deb [ arch=amd64,arm64 signed-by=/usr/share/keyrings/mongodb-server-7.0.gpg ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/7.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-7.0.list\n</code></pre></p> <p>Update package database and install MongoDB: <pre><code>sudo apt-get update\nsudo apt-get install -y mongodb-org\n</code></pre></p> <p>Start and enable MongoDB: <pre><code>sudo systemctl start mongod\nsudo systemctl enable mongod\n</code></pre></p>"},{"location":"products/mobileforge/deployment/database/#postgresql-16-installation","title":"PostgreSQL 16 Installation","text":"<p>Add PostgreSQL repository: <pre><code>sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" &gt; /etc/apt/sources.list.d/pgdg.list'\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\n</code></pre></p> <p>Install PostgreSQL: <pre><code>sudo apt-get update\nsudo apt-get install -y postgresql-16\n</code></pre></p> <p>Start and enable PostgreSQL: <pre><code>sudo systemctl start postgresql\nsudo systemctl enable postgresql\n</code></pre></p>"},{"location":"products/mobileforge/deployment/database/#3-aws-managed-databases-recommended-for-production-environments","title":"3. AWS Managed Databases (Recommended for Production Environments)","text":""},{"location":"products/mobileforge/deployment/database/#amazon-documentdb-mongodb-compatible","title":"Amazon DocumentDB (MongoDB Compatible)","text":"<ol> <li>Create a DocumentDB cluster:</li> <li>Go to AWS Console \u2192 DocumentDB</li> <li>Click \"Create cluster\"</li> <li>Select engine version compatible with MongoDB 7.0</li> <li>Configure instance class (recommended: db.r6g.large or higher)</li> <li>Set up VPC and security groups</li> <li>Configure backup retention period</li> <li> <p>Set up encryption at rest</p> </li> <li> <p>Configure security:</p> </li> <li>Configure security groups to allow access from application servers</li> </ol>"},{"location":"products/mobileforge/deployment/database/#amazon-rds-for-postgresql-16","title":"Amazon RDS for PostgreSQL 16","text":"<ol> <li>Create an RDS instance:</li> <li>Go to AWS Console \u2192 RDS</li> <li>Click \"Create database\"</li> <li>Choose PostgreSQL 16</li> <li>Select instance class (recommended: db.r6g.large or higher)</li> <li>Configure storage (recommended: 100GB+ with autoscaling)</li> <li>Set up VPC and security groups</li> <li>Enable Multi-AZ deployment for high availability</li> <li>Configure backup retention period</li> <li> <p>Enable encryption at rest</p> </li> <li> <p>Configure security:</p> </li> <li>Configure security groups to allow access from application servers</li> </ol>"},{"location":"products/mobileforge/deployment/database/#database-schema-setup","title":"Database Schema Setup","text":""},{"location":"products/mobileforge/deployment/database/#postgresql-database-setup","title":"PostgreSQL Database Setup","text":""},{"location":"products/mobileforge/deployment/database/#required-postgresql-databases","title":"Required PostgreSQL Databases","text":"<ul> <li><code>api_insights_db</code> (main application database)</li> </ul>"},{"location":"products/mobileforge/deployment/database/#create-database-and-user","title":"Create database and user:","text":"<pre><code>-- Create databases\nCREATE DATABASE api_insights_db;\n\n-- Create user with password\nCREATE USER mobileforge_user WITH PASSWORD 'your_secure_password';\n\n-- Grant privileges for api_insights_db\nGRANT CONNECT ON DATABASE api_insights_db TO mobileforge_user;\nGRANT USAGE ON SCHEMA public TO mobileforge_user;\nGRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO mobileforge_user;\nGRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA public TO mobileforge_user;\n\n-- Set default privileges for future tables in both databases\nALTER DEFAULT PRIVILEGES IN SCHEMA public \nGRANT SELECT, INSERT, UPDATE, DELETE ON TABLES TO mobileforge_user;\n\nALTER DEFAULT PRIVILEGES IN SCHEMA public \nGRANT USAGE, SELECT ON SEQUENCES TO mobileforge_user;\n</code></pre>"},{"location":"products/mobileforge/deployment/database/#create-tables","title":"Create Tables","text":"<p>Database to use - <code>api_insights_db</code></p> <pre><code>-- public.clients definition\n\nCREATE TABLE public.clients (\n    id bigint GENERATED ALWAYS AS IDENTITY,\n    client_id varchar(72) NOT NULL,\n    email varchar(32) NULL,\n    auth_token varchar(36) NOT NULL,\n    created_at timestamp DEFAULT CURRENT_TIMESTAMP NOT NULL,\n    updated_at timestamp DEFAULT CURRENT_TIMESTAMP NOT NULL,\n    geography varchar(128) DEFAULT 'india'::character varying NOT NULL,\n    CONSTRAINT clients_pkey PRIMARY KEY (id, created_at)\n);\nCREATE INDEX idx_clients_client_id ON public.clients USING btree (client_id);\n\n-- public.ext_fetch_features_request_dump definition\n\nCREATE TABLE public.ext_fetch_features_request_dump (\n    request_id varchar(128) NOT NULL,\n    user_id varchar(128) NOT NULL,\n    client_id varchar(128) NOT NULL,\n    created_at timestamp DEFAULT now() NOT NULL,\n    cutoff_date varchar(128) NOT NULL,\n    CONSTRAINT ext_fetch_features_request_dump_pkey PRIMARY KEY (request_id, created_at)\n) PARTITION BY RANGE (created_at);\nCREATE INDEX idx_ext_featurize_req_client_id ON public.ext_fetch_features_request_dump USING btree (client_id);\nCREATE INDEX idx_ext_featurize_req_user_id ON public.ext_fetch_features_request_dump USING btree (user_id);\n\nCREATE TABLE public.ext_fetch_features_request_dump_default \n    PARTITION OF public.ext_fetch_features_request_dump \n    FOR VALUES FROM (MINVALUE) TO (MAXVALUE);\n\n-- public.ext_fetch_features_response_dump definition\n\nCREATE TABLE public.ext_fetch_features_response_dump (\n    request_id varchar(128) NOT NULL,\n    user_id varchar(128) NOT NULL,\n    client_id varchar(128) NOT NULL,\n    created_at timestamp DEFAULT now() NOT NULL,\n    response_dump json NULL,\n    groupings json NULL,\n    time_to_featurize float8 NULL,\n    time_json json NULL,\n    CONSTRAINT ext_fetch_features_response_dump_pkey PRIMARY KEY (request_id, created_at)\n) PARTITION BY RANGE (created_at);;\nCREATE INDEX idx_ext_featurize_resp_client_id ON public.ext_fetch_features_response_dump USING btree (client_id);\nCREATE INDEX idx_ext_featurize_resp_user_id ON public.ext_fetch_features_response_dump USING btree (user_id);\n\nCREATE TABLE public.ext_fetch_features_response_dump_default \n    PARTITION OF public.ext_fetch_features_response_dump \n    FOR VALUES FROM (MINVALUE) TO (MAXVALUE);\n\n-- public.fetch_extracted_request_dump definition\n\nCREATE TABLE public.fetch_extracted_request_dump (\n    request_id varchar(128) NOT NULL,\n    user_id varchar(128) NOT NULL,\n    client_id varchar(128) NOT NULL,\n    created_at timestamp DEFAULT now() NOT NULL,\n    cutoff_date varchar(128) NOT NULL,\n    CONSTRAINT fetch_extracted_request_dump_pkey PRIMARY KEY (request_id, created_at)\n) PARTITION BY RANGE (created_at);\nCREATE INDEX idx_fetch_extracted_req_client_id ON public.fetch_extracted_request_dump USING btree (client_id);\nCREATE INDEX idx_fetch_extracted_req_user_id ON public.fetch_extracted_request_dump USING btree (user_id);\n\nCREATE TABLE public.fetch_extracted_request_dump_default \n    PARTITION OF public.fetch_extracted_request_dump \n    FOR VALUES FROM (MINVALUE) TO (MAXVALUE);\n\n-- public.fetch_extracted_response_dump definition\n\nCREATE TABLE public.fetch_extracted_response_dump (\n    request_id varchar(128) NOT NULL,\n    user_id varchar(128) NOT NULL,\n    client_id varchar(128) NOT NULL,\n    created_at timestamp DEFAULT now() NOT NULL,\n    response_dump json NULL,\n    time_to_featurize float8 NULL,\n    time_json json NULL,\n    CONSTRAINT fetch_extracted_response_dump_pkey PRIMARY KEY (request_id, created_at)\n) PARTITION BY RANGE (created_at);\nCREATE INDEX idx_fetch_extracted_resp_client_id ON public.fetch_extracted_response_dump USING btree (client_id);\nCREATE INDEX idx_fetch_extracted_resp_user_id ON public.fetch_extracted_response_dump USING btree (user_id);\n\nCREATE TABLE public.fetch_extracted_response_dump_default \n    PARTITION OF public.fetch_extracted_response_dump \n    FOR VALUES FROM (MINVALUE) TO (MAXVALUE);\n\n-- public.fetch_request_dump definition\n\nCREATE TABLE public.fetch_request_dump (\n    request_id varchar(128) NOT NULL,\n    user_id varchar(128) NOT NULL,\n    client_id varchar(128) NOT NULL,\n    created_at timestamp DEFAULT now() NOT NULL,\n    cutoff_date varchar(128) NOT NULL,\n    CONSTRAINT fetch_request_dump_pkey PRIMARY KEY (request_id, created_at)\n) PARTITION BY RANGE (created_at);\nCREATE INDEX idx_fetch_req_client_id ON public.fetch_request_dump USING btree (client_id);\nCREATE INDEX idx_fetch_req_user_id ON public.fetch_request_dump USING btree (user_id);\n\nCREATE TABLE public.fetch_request_dump_default \n    PARTITION OF public.fetch_request_dump \n    FOR VALUES FROM (MINVALUE) TO (MAXVALUE);\n\n-- public.fetch_response_dump definition\n\nCREATE TABLE public.fetch_response_dump (\n    request_id varchar(128) NOT NULL,\n    user_id varchar(128) NOT NULL,\n    client_id varchar(128) NOT NULL,\n    created_at timestamp DEFAULT now() NOT NULL,\n    response_dump json NULL,\n    time_to_featurize float8 NULL,\n    time_json json NULL,\n    CONSTRAINT fetch_response_dump_pkey PRIMARY KEY (request_id, created_at)\n) PARTITION BY RANGE (created_at);\nCREATE INDEX idx_fetch_resp_client_id ON public.fetch_response_dump USING btree (client_id);\nCREATE INDEX idx_fetch_resp_user_id ON public.fetch_response_dump USING btree (user_id);\n\nCREATE TABLE public.fetch_response_dump_default \n    PARTITION OF public.fetch_response_dump \n    FOR VALUES FROM (MINVALUE) TO (MAXVALUE);\n\n-- public.get_features_request_dump definition\n\nCREATE TABLE public.get_features_request_dump (\n    request_id varchar(128) NOT NULL,\n    user_id varchar(128) NOT NULL,\n    client_id varchar(128) NOT NULL,\n    sms_data json NULL,\n    apps_data json NULL,\n    device_data json NULL,\n    call_data json NULL,\n    contacts_data json NULL,\n    created_at timestamp DEFAULT now() NOT NULL,\n    cutoff_date varchar(128) NOT NULL,\n    CONSTRAINT get_features_request_dump_pkey PRIMARY KEY (request_id, created_at)\n) PARTITION BY RANGE (created_at);\nCREATE INDEX idx_featurize_req_client_id ON public.get_features_request_dump USING btree (client_id);\nCREATE INDEX idx_featurize_req_user_id ON public.get_features_request_dump USING btree (user_id);\n\nCREATE TABLE public.get_features_request_dump_default \n    PARTITION OF public.get_features_request_dump \n    FOR VALUES FROM (MINVALUE) TO (MAXVALUE);\n\n-- public.get_features_response_dump definition\n\nCREATE TABLE public.get_features_response_dump (\n    request_id varchar(128) NOT NULL,\n    user_id varchar(128) NOT NULL,\n    client_id varchar(128) NOT NULL,\n    created_at timestamp DEFAULT now() NOT NULL,\n    response_dump json NULL,\n    groupings json NULL,\n    time_to_featurize float8 NULL,\n    time_json json NULL,\n    CONSTRAINT get_features_response_dump_pkey PRIMARY KEY (request_id, created_at)\n) PARTITION BY RANGE (created_at);\nCREATE INDEX idx_featurize_resp_client_id ON public.get_features_response_dump USING btree (client_id);\nCREATE INDEX idx_featurize_resp_user_id ON public.get_features_response_dump USING btree (user_id);\n\nCREATE TABLE public.get_features_response_dump_default \n    PARTITION OF public.get_features_response_dump \n    FOR VALUES FROM (MINVALUE) TO (MAXVALUE);\n\n-- public.insights_request_dump definition\n\nCREATE TABLE public.insights_request_dump (\n    request_id varchar(128) NOT NULL,\n    user_id varchar(128) NOT NULL,\n    client_id varchar(128) NOT NULL,\n    created_at timestamp DEFAULT now() NOT NULL,\n    cutoff_date varchar(128) NOT NULL,\n    CONSTRAINT insights_request_dump_pkey PRIMARY KEY (request_id, created_at)\n) PARTITION BY RANGE (created_at);\nCREATE INDEX idx_insights_req_client_id ON public.insights_request_dump USING btree (client_id);\nCREATE INDEX idx_insights_req_user_id ON public.insights_request_dump USING btree (user_id);\n\nCREATE TABLE public.insights_request_dump_default \n    PARTITION OF public.insights_request_dump \n    FOR VALUES FROM (MINVALUE) TO (MAXVALUE);\n\n-- public.insights_response_dump definition\n\nCREATE TABLE public.insights_response_dump (\n    request_id varchar(128) NOT NULL,\n    user_id varchar(128) NOT NULL,\n    client_id varchar(128) NOT NULL,\n    created_at timestamp DEFAULT now() NOT NULL,\n    response_dump json NULL,\n    time_to_featurize float8 NULL,\n    time_json json NULL,\n    groupings json DEFAULT '[]'::json NOT NULL,\n    CONSTRAINT insights_response_dump_pkey PRIMARY KEY (request_id, created_at)\n) PARTITION BY RANGE (created_at);\nCREATE INDEX idx_insights_resp_client_id ON public.insights_response_dump USING btree (client_id);\nCREATE INDEX idx_insights_resp_user_id ON public.insights_response_dump USING btree (user_id);\n\nCREATE TABLE public.insights_response_dump_default \n    PARTITION OF public.insights_response_dump \n    FOR VALUES FROM (MINVALUE) TO (MAXVALUE);\n\n-- public.sync_events_log definition\n\nCREATE TABLE public.sync_events_log (\n    id bigint GENERATED ALWAYS AS IDENTITY,\n    event_type varchar(128) NULL,\n    event_value varchar(128) NULL,\n    sync_request_id varchar(128) NULL,\n    client_id varchar(128) NOT NULL,\n    user_id varchar(128) NOT NULL,\n    created_at timestamp DEFAULT now() NOT NULL,\n    CONSTRAINT sync_events_log_pkey PRIMARY KEY (id, created_at)\n) PARTITION BY RANGE (created_at);\nCREATE INDEX idx_sync_events_log_client_id ON public.sync_events_log USING btree (client_id);\nCREATE INDEX idx_sync_events_log_sync_request_id ON public.sync_events_log USING btree (sync_request_id);\nCREATE INDEX idx_sync_events_log_user_id ON public.sync_events_log USING btree (user_id);\n\nCREATE TABLE public.sync_events_log_default \n    PARTITION OF public.sync_events_log \n    FOR VALUES FROM (MINVALUE) TO (MAXVALUE);\n\n-- public.unauthorized_requests definition\n\nCREATE TABLE public.unauthorized_requests (\n    id bigint GENERATED ALWAYS AS IDENTITY,\n    client_id varchar(128) NOT NULL,\n    request_id varchar(128) NOT NULL,\n    auth_token varchar(256) NOT NULL,\n    req_body json NULL,\n    created_at timestamp DEFAULT now() NOT NULL,\n    CONSTRAINT unauthorized_requests_pkey PRIMARY KEY (id, created_at)\n) PARTITION BY RANGE (created_at);\n\nCREATE TABLE public.unauthorized_requests_default \n    PARTITION OF public.unauthorized_requests \n    FOR VALUES FROM (MINVALUE) TO (MAXVALUE);\n\n-- public.\"usage\" definition\n\nCREATE TABLE public.\"usage\" (\n    id bigint GENERATED ALWAYS AS IDENTITY,\n    request_id varchar(128) NULL,\n    client_id varchar(128) NOT NULL,\n    api_name varchar(128) NOT NULL,\n    response_code int4 NULL,\n    created_at timestamp NOT NULL,\n    CONSTRAINT usage_pkey PRIMARY KEY (id, created_at)\n) PARTITION BY RANGE (created_at);\nCREATE INDEX idx_usage_api_name ON public.usage USING btree (api_name);\nCREATE INDEX idx_usage_client_id ON public.usage USING btree (client_id);\n\nCREATE TABLE public.usage_default \n    PARTITION OF public.usage \n    FOR VALUES FROM (MINVALUE) TO (MAXVALUE);\n</code></pre>"},{"location":"products/mobileforge/deployment/database/#mongodb-database-setup","title":"MongoDB Database Setup","text":""},{"location":"products/mobileforge/deployment/database/#create-databases-and-user","title":"Create databases and user","text":"<pre><code>// Connect to MongoDB\nuse admin\n\n// Create application user\ndb.createUser({\n  user: \"mobileforge_user\",\n  pwd: \"your_secure_password\",\n  roles: [\n    { role: \"readWrite\", db: \"sync_db\" }\n  ]\n})\n</code></pre>"},{"location":"products/mobileforge/deployment/database/#create-collections-in-mongodb","title":"Create collections in MongoDB","text":"<pre><code>// Switch to sync_db database\nuse sync_db\n\n// Create collections with indexes (without validators)\n\n// 1. Device Sync Request Device Data\ndb.createCollection(\"device_sync_request_device_data\")\ndb.device_sync_request_device_data.createIndex({ \"client_id\": 1, \"user_id\": 1 })\n\n// 2. Call Logs Sync Request Data\ndb.createCollection(\"call_logs_sync_request_call_logs_data\")\ndb.call_logs_sync_request_call_logs_data.createIndex({ \"client_id\": 1, \"user_id\": 1 })\n\n// 3. Device Sync Request Apps Data\ndb.createCollection(\"device_sync_request_apps_data\")\ndb.device_sync_request_apps_data.createIndex({ \"client_id\": 1, \"user_id\": 1 })\n\n// 4. Contacts Sync Request Data\ndb.createCollection(\"contacts_sync_request_contacts_data\")\ndb.contacts_sync_request_contacts_data.createIndex({ \"client_id\": 1, \"user_id\": 1 })\n\n// 5. Device Transpose\ndb.createCollection(\"device_transpose\")\ndb.device_transpose.createIndex({ \"client_id\": 1 })\ndb.device_transpose.createIndex({ \"value\": 1 })\ndb.device_transpose.createIndex({ \"value_type\": 1 })\n\n// 6. Device Sync Request iOS Device Data\ndb.createCollection(\"device_sync_request_ios_device_data\")\ndb.device_sync_request_ios_device_data.createIndex({ \"client_id\": 1, \"user_id\": 1 })\n\n// 7. SMS Sync Request Data\ndb.createCollection(\"sms_sync_request_sms_data\")\ndb.sms_sync_request_sms_data.createIndex({ \"client_id\": 1, \"user_id\": 1 })\n\n// 8. Device ID Mapper\ndb.createCollection(\"device_id_mapper\")\ndb.device_id_mapper.createIndex({ \"android_id\": 1 })\ndb.device_id_mapper.createIndex({ \"app_device_id\": 1 })\ndb.device_id_mapper.createIndex({ \"device_id\": 1 })\ndb.device_id_mapper.createIndex({ \"assembled_id\": 1 })\ndb.device_id_mapper.createIndex({ \"client_id\": 1 })\ndb.device_id_mapper.createIndex({ \"google_adv_id\": 1 })\n</code></pre>"},{"location":"products/mobileforge/deployment/database/#important-notes","title":"Important Notes","text":"<ol> <li>Replace <code>your_secure_password</code> with a strong, secure password in all scripts</li> <li>Ensure proper network security rules are in place before running these scripts</li> <li>For production environments, use AWS Secrets Manager or similar services to manage credentials</li> <li>Regularly rotate database credentials</li> <li>Monitor database access logs for any unauthorized access attempts</li> </ol>"},{"location":"products/mobileforge/deployment/database/#security-considerations","title":"Security Considerations","text":"<ol> <li>Always use strong passwords</li> <li>Enable encryption at rest</li> <li>Configure network security (firewalls, security groups)</li> <li>Regular backups</li> <li>Monitor database performance and logs</li> <li>Keep databases updated with security patches</li> </ol>"},{"location":"products/mobileforge/deployment/database/#backup-and-recovery","title":"Backup and Recovery","text":""},{"location":"products/mobileforge/deployment/database/#mongodb","title":"MongoDB","text":"<ul> <li>Regular automated backups</li> <li>Point-in-time recovery (for DocumentDB)</li> <li>Backup verification</li> <li>Recovery testing</li> </ul>"},{"location":"products/mobileforge/deployment/database/#postgresql","title":"PostgreSQL","text":"<ul> <li>Automated snapshots</li> <li>Point-in-time recovery</li> <li>WAL archiving</li> <li>Backup verification</li> <li>Recovery testing</li> </ul>"},{"location":"products/mobileforge/deployment/database/#monitoring","title":"Monitoring","text":"<ol> <li>Set up CloudWatch metrics (for AWS deployments)</li> <li>Configure database-specific monitoring tools</li> <li>Set up alerts for:</li> <li>High CPU usage</li> <li>Low disk space</li> <li>Connection count</li> <li>Error rates</li> <li>Replication lag</li> </ol>"},{"location":"products/mobileforge/deployment/database/#maintenance","title":"Maintenance","text":"<ol> <li>Regular security updates</li> <li>Performance optimization</li> <li>Index maintenance</li> <li>Vacuum operations (PostgreSQL)</li> <li>Database statistics updates</li> </ol>"},{"location":"products/mobileforge/deployment/database/#scaling-ladder","title":"Scaling Ladder","text":"<p>The following table provides recommended node specifications and counts for PostgreSQL and MongoDB deployments at various Daily Active User (DAU) scales.</p> <ul> <li>PostgreSQL: 4GB RAM, 2 vCPU, 200GB disk (3000 IOPS, 125 MBPS)</li> <li>MongoDB: 16GB RAM, 4 vCPU, 2TB disk (12000 IOPS, 500 MBPS)</li> </ul> DAU PostgreSQL Nodes MongoDB Nodes 25K 1 1 50K 1-2 1-2 75K 2-3 2-3 100K 3-4 3-4 <p>Notes:</p> <ul> <li>For high availability and failover, consider running at least 2 nodes (primary + standby/replica) at all times, even at lower DAU.</li> <li>Scale up node count and/or instance size if you observe CPU, RAM, or disk IOPS/throughput consistently above 50% utilization.</li> <li>Adjust disk size and IOPS as data volume grows.</li> <li>Monitor database metrics and tune accordingly.</li> </ul>"},{"location":"products/mobileforge/deployment/insights_pipeline/","title":"Deployment: MobileForge Insights Pipeline","text":"<p>This document outlines the deployment steps for different components of the insights pipeline.</p>"},{"location":"products/mobileforge/deployment/insights_pipeline/#components-overview","title":"Components Overview","text":"<p>The sync pipeline consists of three main components -</p> <ol> <li>Insights API</li> <li>SMS Extraction API</li> </ol> <p>Note: All required Docker images for the insights pipeline components will be provided by Credeau via AWS ECR or another designated container registry. Please ensure you have access credentials as required.</p>"},{"location":"products/mobileforge/deployment/insights_pipeline/#1-insights-api-deployment","title":"1. Insights API Deployment","text":""},{"location":"products/mobileforge/deployment/insights_pipeline/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to AWS ECR or other container registry</li> <li>Docker installed on the deployment machine</li> <li>AWS CLI configured (if using AWS ECR)</li> </ul>"},{"location":"products/mobileforge/deployment/insights_pipeline/#deployment-steps","title":"Deployment Steps","text":""},{"location":"products/mobileforge/deployment/insights_pipeline/#pull-the-insights-api-image","title":"Pull the Insights API Image","text":"<pre><code># For AWS ECR\naws ecr get-login-password --region &lt;region&gt; | docker login --username AWS --password-stdin &lt;account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com\ndocker pull &lt;account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/credeau-insights-api:&lt;version&gt;\n</code></pre>"},{"location":"products/mobileforge/deployment/insights_pipeline/#configure-environment-variables","title":"Configure Environment Variables","text":"<p>Create a <code>.env</code> file with the following variables -</p> <pre><code>LAUNCH_ENVIRONMENT=\"prod\"\nDI_POSTGRES_USERNAME=\"mobileforge_user\"\nDI_POSTGRES_PASSWORD=\"your_secure_password\"\nDI_POSTGRES_HOST=\"&lt;host address of deployed PostgresSQL host&gt;\"\nDI_POSTGRES_PORT=\"5432\"\nDI_POSTGRES_DATABASE=\"api_insights_db\"\nDI_POSTGRES_SYNC_DATABASE=\"sync_db\"\nSMS_EXTRACTOR_SERVICE_URL=\"&lt;sms extractor service url&gt;\"\nSMS_EXTRACTOR_BATCH_SIZE=\"1000\"\nDI_MONGODB_USERNAME=\"mobileforge_user\"\nDI_MONGODB_PASSWORD=\"your_secure_password\"\nDI_MONGODB_HOST=\"&lt;host address of deployed MongoDB host&gt;\"\nDI_MONGODB_PORT=\"27017\"\nDI_MONGODB_DATABASE=\"sync_db\"\nDI_MONGODB_ENABLED_SOURCES=\"*\"\nDI_MONGODB_MAX_POOL_SIZE=\"100\"\nDI_MONGODB_MIN_POOL_SIZE=\"10\"\nDI_MONGODB_SERVER_SELECTION_TIMEOUT_MS=\"15000\"\nDI_MONGODB_CONNECT_TIMEOUT_MS=\"10000\"\nDI_MONGODB_SOCKET_TIMEOUT_MS=\"10000\"\nDI_MONGODB_RETRY_WRITES=\"true\"\nDI_MONGODB_WAIT_QUEUE_TIMEOUT_MS=\"2000\"\n</code></pre>"},{"location":"products/mobileforge/deployment/insights_pipeline/#run-the-container","title":"Run the Container","text":"<pre><code>docker run -d \\\n    --name insights-api \\\n    --env-file .env \\\n    -p 8000:8000 \\\n    &lt;account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/credeau-insights-api:&lt;version&gt;\n</code></pre>"},{"location":"products/mobileforge/deployment/insights_pipeline/#nodes-exposure-and-scaling-recommendations","title":"Nodes Exposure and Scaling Recommendations","text":""},{"location":"products/mobileforge/deployment/insights_pipeline/#exposing-nodes-with-load-balancer","title":"Exposing Nodes with Load Balancer","text":"<ul> <li>For production deployments, expose your Insights API service using a load balancer (such as AWS Application Load Balancer or Network Load Balancer).</li> <li>This ensures high availability, fault tolerance, and even distribution of traffic.</li> <li>In Kubernetes, use a <code>Service</code> of type <code>LoadBalancer</code> to expose your pods.</li> <li>For Docker Compose or EC2, place your containers behind an AWS ELB/ALB.</li> </ul>"},{"location":"products/mobileforge/deployment/insights_pipeline/#recommended-node-specifications","title":"Recommended Node Specifications","text":"<ul> <li>Use nodes/EC2 instances with at least 16GB RAM and 4 vCPUs for running Insights API.</li> <li>This ensures sufficient resources for stable operation and avoids out-of-memory or CPU throttling issues.</li> </ul>"},{"location":"products/mobileforge/deployment/insights_pipeline/#scaling-guidance","title":"Scaling Guidance","text":"<ul> <li>Monitor CPU and RAM usage for each service.</li> <li>Scale up (add more pods/containers/instances) if max CPU or RAM usage exceeds 50% for a period of 1 minute.</li> </ul>"},{"location":"products/mobileforge/deployment/insights_pipeline/#2-sms-extraction-deployment","title":"2. SMS Extraction Deployment","text":""},{"location":"products/mobileforge/deployment/insights_pipeline/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Access to AWS ECR or other container registry</li> <li>Docker installed on the deployment machine</li> <li>AWS CLI configured (if using AWS ECR)</li> </ul>"},{"location":"products/mobileforge/deployment/insights_pipeline/#deployment-steps_1","title":"Deployment Steps","text":""},{"location":"products/mobileforge/deployment/insights_pipeline/#pull-the-sms-extraction-api-image","title":"Pull the SMS Extraction API Image","text":"<pre><code># For AWS ECR\naws ecr get-login-password --region &lt;region&gt; | docker login --username AWS --password-stdin &lt;account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com\ndocker pull &lt;account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/credeau-sms-extraction:&lt;version&gt;\n</code></pre>"},{"location":"products/mobileforge/deployment/insights_pipeline/#configure-environment-variables_1","title":"Configure Environment Variables","text":"<p>Create a <code>.env</code> file with the following variables -</p> <pre><code>DB_USER=\"mobileforge_user\"\nDB_PASSWORD=\"your_secure_password\"\nDB_HOST=\"&lt;host address of deployed PostgresSQL host&gt;\"\nDB_PORT=\"5432\"\nDB_NAME=\"api_insights_db\"\nMAX_SMS_COUNT=\"1000\"\nDB_ENCRYPTION_KEY=\"&lt;db_encryption_key&gt;\"\n</code></pre>"},{"location":"products/mobileforge/deployment/insights_pipeline/#run-the-container_1","title":"Run the Container","text":"<pre><code>docker run -d \\\n    --name sms-extraction \\\n    --env-file .env \\\n    &lt;account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/credeau-sms-extraction:&lt;version&gt;\n</code></pre>"},{"location":"products/mobileforge/deployment/insights_pipeline/#scaling-recommendations","title":"Scaling Recommendations","text":""},{"location":"products/mobileforge/deployment/insights_pipeline/#recommended-node-specifications_1","title":"Recommended Node Specifications","text":"<ul> <li>Use nodes/EC2 instances with at least 16GB RAM and 4 vCPUs for running SMS Extraction API.</li> <li>This ensures sufficient resources for stable operation and avoids out-of-memory or CPU throttling issues.</li> </ul>"},{"location":"products/mobileforge/deployment/insights_pipeline/#scaling-guidance_1","title":"Scaling Guidance","text":"<ul> <li>Monitor CPU and RAM usage for each service.</li> <li>Scale up (add more pods/containers/instances) if max CPU or RAM usage exceeds 50% for a period of 1 minute.</li> </ul>"},{"location":"products/mobileforge/deployment/insights_pipeline/#scaling-ladder","title":"Scaling Ladder","text":"<p>The following table provides recommended node counts based on daily active users (DAU):</p> Daily Active Users Insights API Nodes SMS Extraction API Nodes 25K 1-2 1 50K 2-5 1-3 75K 4-7 2-4 100K 6-10 3-5 <p>Note: These recommendations assume:</p> <ul> <li>Each node has the minimum recommended specifications (16GB RAM, 4 vCPUs)</li> <li>Average user activity patterns</li> <li>Standard business hours usage</li> <li>Regular maintenance windows</li> </ul> <p>Adjust node counts based on:</p> <ul> <li>Peak usage times</li> <li>Geographic distribution of users</li> <li>Specific workload patterns</li> <li>Performance monitoring metrics</li> </ul>"},{"location":"products/mobileforge/deployment/sync_pipeline/","title":"Deployment: MobileForge Sync Pipeline","text":"<p>This document outlines the deployment steps for different components of the sync pipeline.</p>"},{"location":"products/mobileforge/deployment/sync_pipeline/#components-overview","title":"Components Overview","text":"<p>The sync pipeline consists of three main components -</p> <ol> <li>Producer API</li> <li>Kafka Message Broker</li> <li>Consumer Service</li> </ol> <p>Note: All required Docker images for the sync pipeline components will be provided by Credeau via AWS ECR or another designated container registry. Please ensure you have access credentials as required.</p>"},{"location":"products/mobileforge/deployment/sync_pipeline/#1-producer-api-deployment","title":"1. Producer API Deployment","text":""},{"location":"products/mobileforge/deployment/sync_pipeline/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to AWS ECR or other container registry</li> <li>Docker installed on the deployment machine</li> <li>AWS CLI configured (if using AWS ECR)</li> </ul>"},{"location":"products/mobileforge/deployment/sync_pipeline/#deployment-steps","title":"Deployment Steps","text":""},{"location":"products/mobileforge/deployment/sync_pipeline/#pull-the-producer-api-image","title":"Pull the Producer API Image","text":"<pre><code># For AWS ECR\naws ecr get-login-password --region &lt;region&gt; | docker login --username AWS --password-stdin &lt;account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com\ndocker pull &lt;account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/credeau-producer-api:&lt;version&gt;\n</code></pre>"},{"location":"products/mobileforge/deployment/sync_pipeline/#configure-environment-variables","title":"Configure Environment Variables","text":"<p>Create a <code>.env</code> file with the following variables -</p> <pre><code>DI_POSTGRES_USERNAME=\"mobileforge_user\"\nDI_POSTGRES_PASSWORD=\"your_secure_password\"\nDI_POSTGRES_HOST=\"&lt;host address of deployed PostgresSQL host&gt;\"\nDI_POSTGRES_PORT=\"5432\"\nDI_POSTGRES_DATABASE=\"api_insights_db\"\nDI_POSTGRES_SYNC_DATABASE=\"sync_db\"\nDI_MONGODB_USERNAME=\"mobileforge_user\"\nDI_MONGODB_PASSWORD=\"your_secure_password\"\nDI_MONGODB_HOST=\"&lt;host address of deployed MongoDB host&gt;\"\nDI_MONGODB_PORT=\"27017\"\nDI_MONGODB_DATABASE=\"sync_db\"\nDI_MONGODB_ENABLED_SOURCES=\"*\"\nDI_MONGODB_MAX_POOL_SIZE=\"100\"\nDI_MONGODB_MIN_POOL_SIZE=\"10\"\nDI_MONGODB_SERVER_SELECTION_TIMEOUT_MS=\"15000\"\nDI_MONGODB_CONNECT_TIMEOUT_MS=\"10000\"\nDI_MONGODB_SOCKET_TIMEOUT_MS=\"10000\"\nDI_MONGODB_RETRY_WRITES=\"true\"\nDI_MONGODB_WAIT_QUEUE_TIMEOUT_MS=\"2000\"\nDI_KAFKA_BROKER_ENDPOINT=\"&lt;bootstrap-server-1&gt;:9092,&lt;bootstrap-server-2&gt;:9092,&lt;bootstrap-server-3&gt;:9092\"\n</code></pre>"},{"location":"products/mobileforge/deployment/sync_pipeline/#run-the-container","title":"Run the Container","text":"<pre><code>docker run -d \\\n    --name producer-api \\\n    --env-file .env \\\n    -p 8000:8000 \\\n    &lt;account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/credeau-producer-api:&lt;version&gt;\n</code></pre>"},{"location":"products/mobileforge/deployment/sync_pipeline/#nodes-exposure-and-scaling-recommendations","title":"Nodes Exposure and Scaling Recommendations","text":""},{"location":"products/mobileforge/deployment/sync_pipeline/#exposing-nodes-with-load-balancer","title":"Exposing Nodes with Load Balancer","text":"<ul> <li>For production deployments, expose your Producer API service using a load balancer (such as AWS Application Load Balancer or Network Load Balancer).</li> <li>This ensures high availability, fault tolerance, and even distribution of traffic.</li> <li>In Kubernetes, use a <code>Service</code> of type <code>LoadBalancer</code> to expose your pods.</li> <li>For Docker Compose or EC2, place your containers behind an AWS ELB/ALB.</li> </ul>"},{"location":"products/mobileforge/deployment/sync_pipeline/#recommended-node-specifications","title":"Recommended Node Specifications","text":"<ul> <li>Use nodes/EC2 instances with at least 2GB RAM and 2 vCPUs for running Producer API.</li> <li>This ensures sufficient resources for stable operation and avoids out-of-memory or CPU throttling issues.</li> </ul>"},{"location":"products/mobileforge/deployment/sync_pipeline/#scaling-guidance","title":"Scaling Guidance","text":"<ul> <li>Monitor CPU and RAM usage for each service.</li> <li>Scale up (add more pods/containers/instances) if max CPU or RAM usage exceeds 50% for a period of 1 minute.</li> </ul>"},{"location":"products/mobileforge/deployment/sync_pipeline/#2-kafka-deployment","title":"2. Kafka Deployment","text":""},{"location":"products/mobileforge/deployment/sync_pipeline/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster or Docker Compose</li> <li>Sufficient storage for message persistence</li> <li>Network access between components</li> </ul>"},{"location":"products/mobileforge/deployment/sync_pipeline/#deployment-methods","title":"Deployment Methods","text":""},{"location":"products/mobileforge/deployment/sync_pipeline/#1-using-docker-compose-recommended-for-dev-testing-uat-environments","title":"1. Using Docker Compose (Recommended for Dev Testing / UAT Environments)","text":"<pre><code>version: '3'\nservices:\n    zookeeper:\n    image: confluentinc/cp-zookeeper:latest\n    environment:\n        ZOOKEEPER_CLIENT_PORT: 2181\n    ports:\n        - \"2181:2181\"\n\n    kafka:\n    image: confluentinc/cp-kafka:latest\n    depends_on:\n        - zookeeper\n    ports:\n        - \"9092:9092\"\n    environment:\n        KAFKA_BROKER_ID: 1\n        KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n        KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092\n        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n        KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n        KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n</code></pre>"},{"location":"products/mobileforge/deployment/sync_pipeline/#2-aws-managed-kafka-recommended-for-production-environments","title":"2. AWS Managed Kafka (Recommended for Production Environments)","text":"<p>Prerequisites</p> <ul> <li>AWS CLI configured with appropriate permissions</li> <li>VPC with at least 2 private subnets in different Availability Zones</li> <li>Security groups for MSK cluster</li> <li>IAM roles and policies for MSK</li> </ul> <p>Create Security Groups</p> <pre><code># Create security group for MSK\naws ec2 create-security-group \\\n    --group-name msk-security-group \\\n    --description \"Security group for MSK cluster\" \\\n    --vpc-id &lt;your-vpc-id&gt;\n\n# Add inbound rules for Kafka\naws ec2 authorize-security-group-ingress \\\n    --group-id &lt;msk-security-group-id&gt; \\\n    --protocol tcp \\\n    --port 9092 \\\n    --source-group &lt;producer-consumer-security-group-id&gt;\n\naws ec2 authorize-security-group-ingress \\\n    --group-id &lt;msk-security-group-id&gt; \\\n    --protocol tcp \\\n    --port 2181 \\\n    --source-group &lt;producer-consumer-security-group-id&gt;\n</code></pre> <p>Create MSK Cluster</p> <pre><code>aws kafka create-cluster \\\n    --cluster-name \"mobileforge-sync-cluster\" \\\n    --kafka-version \"3.8.0\" \\\n    --number-of-broker-nodes 3 \\\n    --enhanced-monitoring PER_BROKER \\\n    --broker-node-group-info \\\n    '{\n        \"ClientSubnets\": [\"&lt;subnet-id-1&gt;\", \"&lt;subnet-id-2&gt;\", \"&lt;subnet-id-3&gt;\"],\n        \"SecurityGroups\": [\"&lt;msk-security-group-id&gt;\"],\n        \"InstanceType\": \"kafka.t3a.large\",\n        \"StorageInfo\": {\n            \"EbsStorageInfo\": {\n                \"VolumeSize\": 100\n            }\n        }\n    }' \\\n    --encryption-info \\\n    '{\n        \"EncryptionInTransit\": {\n            \"ClientBroker\": \"TLS\",\n            \"InCluster\": true\n        }\n    }'\n</code></pre> <p>Wait for Cluster Creation</p> <pre><code># Check cluster status\naws kafka describe-cluster --cluster-arn &lt;cluster-arn&gt;\n</code></pre> <p>Get Bootstrap Servers</p> <pre><code># Get bootstrap servers for client configuration\naws kafka get-bootstrap-brokers --cluster-arn &lt;cluster-arn&gt;\n</code></pre> <p>Create Topics</p> <pre><code># Create the sync topic\naws kafka create-topic \\\n    --cluster-arn &lt;cluster-arn&gt; \\\n    --topic sms_batched \\\n    --partitions 16 \\\n    --replication-factor 3 \\\n    --config retention.ms=1800000\n\naws kafka create-topic \\\n    --cluster-arn &lt;cluster-arn&gt; \\\n    --topic events_log \\\n    --partitions 20 \\\n    --replication-factor 3 \\\n    --config retention.ms=1800000\n\naws kafka create-topic \\\n    --cluster-arn &lt;cluster-arn&gt; \\\n    --topic apps_and_device_batched \\\n    --partitions 5 \\\n    --replication-factor 3 \\\n    --config retention.ms=1800000\n\naws kafka create-topic \\\n    --cluster-arn &lt;cluster-arn&gt; \\\n    --topic contacts_batched \\\n    --partitions 5 \\\n    --replication-factor 3 \\\n    --config retention.ms=1800000\n\naws kafka create-topic \\\n    --cluster-arn &lt;cluster-arn&gt; \\\n    --topic call_logs_batched \\\n    --partitions 5 \\\n    --replication-factor 3 \\\n    --config retention.ms=1800000\n</code></pre> <p>Configuration for Producer and Consumer</p> <p>Update the environment variables in your <code>.env</code> files to use the MSK bootstrap servers:</p> <pre><code># For Producer and Consumer .env files\nDI_KAFKA_BROKER_ENDPOINT=\"&lt;bootstrap-server-1&gt;:9092,&lt;bootstrap-server-2&gt;:9092,&lt;bootstrap-server-3&gt;:9092\"\n</code></pre>"},{"location":"products/mobileforge/deployment/sync_pipeline/#cost-optimization","title":"Cost Optimization","text":"<ol> <li>Instance Types</li> <li>Use appropriate instance types based on workload</li> <li>Consider reserved instances for production</li> <li> <p>Monitor broker metrics for right-sizing</p> </li> <li> <p>Storage</p> </li> <li>Configure appropriate EBS volume sizes</li> <li>Monitor storage usage</li> <li>Set up alerts for storage thresholds</li> </ol>"},{"location":"products/mobileforge/deployment/sync_pipeline/#3-consumer-deployment","title":"3. Consumer Deployment","text":""},{"location":"products/mobileforge/deployment/sync_pipeline/#prerequisites_2","title":"Prerequisites","text":"<ul> <li>Access to AWS ECR or other container registry</li> <li>Docker installed on the deployment machine</li> <li>AWS CLI configured (if using AWS ECR)</li> </ul>"},{"location":"products/mobileforge/deployment/sync_pipeline/#deployment-steps_1","title":"Deployment Steps","text":""},{"location":"products/mobileforge/deployment/sync_pipeline/#pull-the-consumer-api-image","title":"Pull the Consumer API Image","text":"<pre><code># For AWS ECR\naws ecr get-login-password --region &lt;region&gt; | docker login --username AWS --password-stdin &lt;account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com\ndocker pull &lt;account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/credeau-consumer:&lt;version&gt;\n</code></pre>"},{"location":"products/mobileforge/deployment/sync_pipeline/#configure-environment-variables_1","title":"Configure Environment Variables","text":"<p>Create a <code>.env</code> file with the following variables -</p> <pre><code>DI_POSTGRES_USERNAME=\"mobileforge_user\"\nDI_POSTGRES_PASSWORD=\"your_secure_password\"\nDI_POSTGRES_HOST=\"&lt;host address of deployed PostgresSQL host&gt;\"\nDI_POSTGRES_PORT=\"5432\"\nDI_POSTGRES_DATABASE=\"api_insights_db\"\nDI_POSTGRES_SYNC_DATABASE=\"sync_db\"\nDI_MONGODB_USERNAME=\"mobileforge_user\"\nDI_MONGODB_PASSWORD=\"your_secure_password\"\nDI_MONGODB_HOST=\"&lt;host address of deployed MongoDB host&gt;\"\nDI_MONGODB_PORT=\"27017\"\nDI_MONGODB_DATABASE=\"sync_db\"\nDI_MONGODB_ENABLED_SOURCES=\"*\"\nDI_MONGODB_MAX_POOL_SIZE=\"100\"\nDI_MONGODB_MIN_POOL_SIZE=\"10\"\nDI_MONGODB_SERVER_SELECTION_TIMEOUT_MS=\"15000\"\nDI_MONGODB_CONNECT_TIMEOUT_MS=\"10000\"\nDI_MONGODB_SOCKET_TIMEOUT_MS=\"10000\"\nDI_MONGODB_RETRY_WRITES=\"true\"\nDI_MONGODB_WAIT_QUEUE_TIMEOUT_MS=\"2000\"\nDI_KAFKA_BROKER_ENDPOINT=\"&lt;bootstrap-server-1&gt;:9092,&lt;bootstrap-server-2&gt;:9092,&lt;bootstrap-server-3&gt;:9092\"\nENABLED_TOPICS=\"sms_batched,apps_and_device_batched,contacts_batched,call_logs_batched,events_log\"\nKAFKA_CONSUMER_GROUP=\"some-consumer-group\"\n</code></pre>"},{"location":"products/mobileforge/deployment/sync_pipeline/#run-the-container_1","title":"Run the Container","text":"<pre><code>docker run -d \\\n    --name consumer \\\n    --env-file .env \\\n    &lt;account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/credeau-consumer:&lt;version&gt;\n</code></pre> <p>For production setups, split consumer into 3 different consumers sms consumer, events consumer and a common consumer. Change the following configurations to achieve this -</p> <p>SMS Consumer -</p> <pre><code>ENABLED_TOPICS=sms_batched\nKAFKA_CONSUMER_GROUP=sms-consumer-group\n</code></pre> <p>Events Consumer -</p> <pre><code>ENABLED_TOPICS=events_log\nKAFKA_CONSUMER_GROUP=events-consumer-group\n</code></pre> <p>Common Consumer -</p> <pre><code>ENABLED_TOPICS=apps_and_device_batched,contacts_batched,call_logs_batched\nKAFKA_CONSUMER_GROUP=common-consumer-group\n</code></pre>"},{"location":"products/mobileforge/deployment/sync_pipeline/#scaling-recommendations","title":"Scaling Recommendations","text":""},{"location":"products/mobileforge/deployment/sync_pipeline/#recommended-node-specifications_1","title":"Recommended Node Specifications","text":"<ul> <li>Use nodes/EC2 instances with at least 2GB RAM and 2 vCPUs for running Producer API.</li> <li>This ensures sufficient resources for stable operation and avoids out-of-memory or CPU throttling issues.</li> </ul>"},{"location":"products/mobileforge/deployment/sync_pipeline/#scaling-guidance_1","title":"Scaling Guidance","text":"<ul> <li>Monitor CPU and RAM usage for each service.</li> <li>Scale up (add more pods/containers/instances) if max CPU or RAM usage exceeds 50% for a period of 1 minute.</li> <li>For more enhanced scaling, monitor Kafka topic lags and trigger scale up as the lag increases more than 50 </li> </ul>"},{"location":"products/mobileforge/deployment/sync_pipeline/#scaling-ladder","title":"Scaling Ladder","text":"DAU Producer Nodes SMS Consumer Events Consumer Common Consumer Kafka Brokers 25K 1-3 1-4 1-2 1-2 1-3 50K 2-5 2-8 1-5 1-5 3 75K 3-7 3-12 2-7 2-7 3 100K 4-10 4-16 3-10 3-10 3-5 <p>With increasing data consumer nodes, do adjust the partition count of the respective topics to at least match the max number of nodes expected.</p>"},{"location":"products/mobilegator/","title":"MobileGator","text":"<p>MobileGator is our cloud-hosted SaaS solution that provides secure and efficient mobile data collection and processing capabilities. It offers a fully managed service where data is securely transmitted to our cloud infrastructure, where profiling and processing take place \u2014 ensuring enterprise-grade security and compliance while eliminating the need for on-premises deployment.</p>"},{"location":"products/mobilegator/#key-features","title":"Key Features","text":"<ul> <li>Fully managed cloud service</li> <li>Enterprise-grade security and compliance</li> <li>Seamless data synchronization</li> <li>Advanced analytics and user profiling capabilities</li> <li>Optimized cloud infrastructure for performance and reliability</li> <li>Built-in monitoring and alerting</li> <li>Automatic scaling and maintenance</li> </ul>"},{"location":"products/mobilegator/sdk/web/","title":"WebSDK: MobileGator Web Analytics SDK","text":"<p>The Web Analytics SDK enables comprehensive collection of browser telemetry, device information, and fraud detection metrics from web applications while maintaining user privacy and compliance.</p>"},{"location":"products/mobilegator/sdk/web/#requirements","title":"Requirements","text":"<p>The Web SDK works on Google Chrome, Safari, Firefox, Opera and other popular modern browsers.</p>"},{"location":"products/mobilegator/sdk/web/#adding-dependency","title":"Adding Dependency","text":"<p>Add the SDK to your application using one of the following methods.</p> <p>Using npm:</p> <pre><code>npm install credLibKit-web\n</code></pre> <p>or using yarn:</p> <pre><code>yarn add credLibKit-web\n</code></pre> <p>CDN Integration - Alternatively, include the SDK via CDN:</p> <pre><code>&lt;script  src=\"https://cdn.jsdelivr.net/npm/credLibKit-web/dist/web-sdk.min.js\"&gt;&lt;/script&gt;\n</code></pre> <p>ES6 Module:</p> <pre><code>import { syncWebData } from \"credLibKit-web\";\n</code></pre> <p>Note:</p> <p>Following credentials will be shared by the Credeau team at the time of integration -</p> <ul> <li><code>CLIENT_ID</code></li> <li><code>SECRET_KEY</code></li> </ul>"},{"location":"products/mobilegator/sdk/web/#function-reference","title":"Function Reference","text":""},{"location":"products/mobilegator/sdk/web/#sync-web-data","title":"Sync Web Data","text":"<p>To collect and synchronize comprehensive browser and device data, call the syncWebData method with the following -</p> <p><code>credLibKit.syncWebData(user_id, client_id, secret_key,\u200bbase_url)</code></p>"},{"location":"products/mobilegator/sdk/web/#parameters","title":"Parameters","text":"Parameter Type Required Description user_id string Yes Unique identifier for the user client_id string Yes Your application's client identifier secret_key string Yes Authentication key for API access base_url string No Custom API endpoint (default: https://devdevicesense.credeau.com) <p>Note:</p> <p><code>user_id</code> must be -</p> <ul> <li>Alphanumeric (no special characters)</li> <li>Should not exceed 64 characters</li> <li>A non-empty / non-null value</li> </ul>"},{"location":"products/mobilegator/sdk/web/#returns","title":"Returns","text":"<p>Returns a Promise that resolves to an object containing all collected data, or null if an error occurs.</p> <p>Response schema -</p> <pre><code>{\n    user_id: \"user123\",\n    client_id: \"client456\",\n    app_device_id: \"unique_device_id\",\n    sync_id: \"sync_identifier\",\n    sdk_version: \"1.0.0\",\n    sync_request_id: \"uuid\",\n\n    // Location\n    latitude: 37.7749,\n    longitude: -122.4194,\n    accuracy: 10,\n    timezone: \"America/Los_Angeles\",\n\n    // Browser\n    browserName: \"Chrome\",\n    os: \"Windows 10\",\n    language: \"en-US\",\n    platform: \"Win32\",\n    vendor: \"Google Inc.\",\n    browser_fingerprint: \"hash_value\",\n\n    // Hardware\n    screenResolution: \"1920x1080\",\n    deviceMemory: 8,\n    logicalProcessors: 8,\n    maxTouchPoints: 0, touchSupport: false,\n    batteryLevel: 0.85,\n    batteryCharging: true,\n\n    // Performance\n    uptimeMs: 1234567,\n\n    // Storage\n    storageUsed: 1024000,\n    storageTotal: 2048000,\n    localStorageAvailable: true,\n    sessionStorageAvailable: true,\n    indexedDBAvailable: true,\n\n    // Media\n    audioInputs: 1,\n    videoInputs: 1,\n    videoCard: \"NVIDIA GeForce GTX 1060\",\n\n    // Security\n    incognito: false,\n    hasAutomationTools: false,\n    isHeadless: false,\n    resistFingerprinting: false,\n    hasPrivacyExtensions: false,\n    doNotTrack: false,\n    clipboardSupport: true\n}\n</code></pre>"},{"location":"products/mobilegator/sdk/web/#response-handling","title":"Response Handling","text":"<p>The response to this method (success or failure) can be captured using async/await or promises.</p> <pre><code>try {\n\n    const result = await credLibKit.syncWebData(\n        \"USER_ID\", // User identifier\n        \"CLIENT_ID\", // Your client identifier\n        \"SECRET_KEY\", // Authentication key\n        \"https://api.yourserver.com\" // Optional: Custom API endpoint\n    );\n\n    if (result) {\n        // Data collection successful console.log(\"Sync successful:\", result);\n    } else {\n        // Data collection failed console.log(\"Sync failed\");\n    }\n} catch (error) {\n    // Handle errors\n    console.error(\"SDK Error:\", error);\n}\n</code></pre>"},{"location":"products/mobilegator/sdk/web/#error-handling","title":"Error Handling","text":"<p>Common error scenarios and their handling -</p> <pre><code>// Handle permission denials gracefully\nif (!result.latitude) {\n    console.log(\"Location access denied by user\");\n}\n</code></pre> <pre><code>// Network connectivity issues\nif (!result) {\n    console.log(\"Network error - sync failed\");\n}\n</code></pre>"},{"location":"products/mobilegator/sdk/web/#privacy-consideration","title":"Privacy Consideration","text":"<ul> <li>Respects user privacy settings and browser limitations</li> <li>Handles incognito/private browsing modes appropriately</li> <li>Complies with Do Not Track preferences</li> <li>No sensitive personal data collected without explicit permission</li> </ul>"}]}